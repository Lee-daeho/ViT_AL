import torch
import torch.nn as nn
import numpy as np

from einops.layers.torch import Rearrange

class Attention(nn.Module):
    pass


class Embeddings(nn.Module):
    def __init__(self, num_patches, img_size, patch_size, hidden_size, patch_embed=1, channels=3):
        super(Embeddings, self).__init__()
        self.num_patches = num_patches
        self.img_size = img_size
        self.patch_size =patch_size
        self.hidden_size = hidden_size
        self.patch_embed = patch_embed
        self.channels = channels

        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, hidden_size))

        if self.patch_embed == 1:
            self.patch_embedding = nn.Sequential(
                Rearrange('b c (h p) (w p) -> b (h w) (p p c)', p=patch_size),
                nn.Linear(patch_size * patch_size * 3 , hidden_size)
            )
        elif self.patch_embed == 2:
            self.patch_embedding = nn.Conv2d(channels, hidden_size, patch_size, patch_size)

        #NEED TO UPDATE HYBRID VERSION

    def forward(self, x):
        batch_size = x.shape[0]     # B * img_size * img_size * channels B * 

        if self.cls_token.shape[0] != batch_size:
            cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        
        self.patch_embedding(x)
        


class Encoder(nn.Module):
    def __init__(self, )


class Transformer(nn.Module):
    def __init__(self, img_size, patch_size, num_heads, hidden_size, num_layers):
        super(Transformer, self).__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_heads = num_heads
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.num_patches = self.img_size // self.patch_size

        self.embeddings = Embeddings()
        self.Encoder = Encoder()



class Vit(nn.Module):
    def __init__(self, img_size, num_classes, patch_size, num_heads, hidden_size, num_layers):
        super(Vit, self).__init__()
        self.img_size = img_size
        self.num_classes = num_classes
        self.patch_size = patch_size
        self.num_heads = num_heads
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        assert img_size % patch_size == 0

        self.transformer = Transformer(img_size, patch_size, num_heads, hidden_size, num_layers)
        self.mlp_head = nn.Linear(hidden_size, num_classes)
